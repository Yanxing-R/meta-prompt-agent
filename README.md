# meta-prompt-agent 🚀

**一个帮助你更好地思考并向大型语言模型（LLM）提出更优问题的智能工具。**

---

## 🌟 项目简介 (Introduction)

`meta-prompt-agent` 旨在成为您的个人提示词工程师助手。在与AI（尤其是大型语言模型）交互时，提问的质量直接影响回答的质量。本项目致力于通过优化和完善用户最初的提问思路，生成结构更清晰、意图更明确、更容易被LLM理解和高效执行的提示词，从而帮助用户获得更高质量的AI输出。

我们相信，更好的提问能够解锁AI更强大的潜能。本项目希望赋能每一位使用者，无论您是经验丰富的AI玩家还是刚刚入门的新手，都能更轻松地与AI进行高效沟通。

## ✨ 核心功能 (Core Features)

* **提示词优化与完善：** 将用户初步的、可能较为模糊的请求，转化为结构化、具体化的高质量提示词。
* **任务类型适应：** 能够针对不同类型的任务（如内容创作、代码生成、数据分析、图像生成等）提供定制化的提示词优化策略。
* **结构化模板支持：** 内置多种针对特定场景的元提示模板，方便用户快速生成针对性强的优化提示。
* **自我校正与递归优化：** 核心逻辑包括让一个LLM充当元提示工程师对用户的提示词进行优化，另一个LLM对优化后的提示词进行评估，这个循环可以根据用户设置进行多次，以迭代提升提示词质量。
* **本地化运行：** 基于 Ollama 在用户本地运行，保障数据隐私与便捷性。
* **用户友好界面：** 通过 Streamlit 提供简洁易用的Web用户界面。

## 🎯 目标用户 (Target Audience)

本项目面向所有对提升与大型语言模型交互效率和质量感兴趣的人，包括但不限于：

* 内容创作者
* 开发者与程序员
* 研究人员与学生
* 产品经理与设计师
* 任何希望通过AI提升工作效率和创造力的个人

## 🛠️ 技术栈 (Tech Stack)

* **主要语言：** Python
* **核心逻辑：** 通过精心设计的元提示模板引导本地LLM（通过Ollama）扮演“元提示工程师”和“提示评估师”的角色，对用户输入进行多轮分析、优化和迭代。
* **用户界面：** Streamlit
* **模型交互：** Ollama (当前默认使用 `qwen3:4b` 模型)
* **包管理：** PDM

## 🚀 安装与环境配置 (Installation & Setup)

**先决条件 (Prerequisites):**

1.  **Python:** 版本 3.9 或更高。
2.  **PDM:** 用于项目依赖管理。如果尚未安装，请参照 [PDM官方文档](https://pdm-project.org/) 进行安装。
3.  **Ollama:** 用于在本地运行大型语言模型。请从 [Ollama官网](https://ollama.com/) 下载并安装适合您操作系统的版本。
    * 安装完成后，请确保Ollama服务正在运行。
    * 通过Ollama拉取本项目默认使用的模型：
        ```bash
        ollama pull qwen3:4b
        ```
        (如果您希望使用其他模型，请相应修改 `agent_logic.py` 中的 `OLLAMA_MODEL` 变量，并确保已通过Ollama拉取该模型。注意：不同模型对提示词的敏感度和理解能力可能存在差异。)

**安装步骤 (Installation Steps):**

1.  **克隆本仓库：**
    ```bash
    git clone [https://github.com/Yanxing-R/meta-prompt-agent.git](https://github.com/Yanxing-R/meta-prompt-agent.git)
    cd meta-prompt-agent
    ```

2.  **安装项目依赖：**
    使用 PDM 安装项目所需的依赖库。PDM会自动读取 `pdm.lock` 文件（如果存在）或 `pyproject.toml` 文件来创建虚拟环境并安装包。
    ```bash
    pdm install
    ```
    或者，如果您更喜欢显式激活PDM管理的虚拟环境（可选）：
    ```bash
    pdm venv activate # 根据您的shell可能有所不同，或使用 pdm shell
    pdm install
    ```

## 💡 使用方法 (Usage)

1.  **启动 Streamlit 应用：**
    在项目根目录下，通过PDM运行 `app.py`：
    ```bash
    pdm run streamlit run src/meta_prompt_agent/app/main_ui.py
    ```
    或者，如果您已经激活了PDM的虚拟环境：
    ```bash
    streamlit run src/meta_prompt_agent/app/main_ui.py
    ```

2.  **打开浏览器：**
    应用启动后，通常会自动在您的默认浏览器中打开。如果没有，请访问终端中显示的本地URL (通常是 `http://localhost:8501`)。

3.  **操作流程：**
    * 在侧边栏选择合适的“任务类型”。
    * （可选）选择一个“结构化模板”并填写其所需的变量。
    * 在主区域输入您的“初步请求”。
    * （可选）配置“自我校正”和“最大递归深度”。
    * 点击“生成优化提示词”按钮。
    * 在下方查看生成的“最终优化后的目标提示词”以及详细的处理日志。

## 🤝 如何贡献 (Contributing)

我们非常欢迎社区的贡献！如果您有任何改进建议、功能需求或发现了Bug，请随时通过以下方式参与：

* 提交 [GitHub Issues](https://github.com/Yanxing-R/meta-prompt-agent/issues)
* 创建 Pull Requests (请确保您的代码遵循项目规范并通过相关检查)

在提交PR前，建议先创建一个Issue来讨论您的想法。

## 🗺️ 未来计划 (Roadmap)

我们的目标是持续迭代，将 `meta-prompt-agent` 打造成一个更强大、更智能、更易用的工具。以下是我们正在考虑的一些方向：

* **模型支持与灵活性：**
    * [ ] 增加对更多本地LLM模型的支持和动态选择功能。
    * [ ] 集成外部更强大的LLM API服务（例如 OpenAI, Anthropic等），供用户选择。
    * [ ] 提供更深度的定制化选项（例如：为不同处理阶段选择不同模型、自定义核心提示模板）。
* **功能增强与智能化：**
    * [x] 引入更高级的提示词评估指标和反馈机制。(还有一些想法未来会继续更新)
    * [ ] 完善用户反馈数据的收集与分析，用于持续改进代理性能。
    * [ ] 引入用户历史交互记录，用于个性化提示词优化。
    * [ ] 集成AI辅助解释功能（例如：对生成提示词中的术语进行划词解释）。
* **用户体验与集成：**
    * [ ] 探索使用现代JavaScript框架（如 React, Vue）重构和美化用户界面。
    * [ ] 提供API接口，使`meta-prompt-agent`能被其他应用或Agent调用和集成（A2A, MCP）。
    * [ ] 提供更详细的自定义模板创建指南。
* **应用场景扩展：**
    * [ ] 扩展和深化对更多特定应用场景的支持，提供更丰富的预设模板和优化策略。

您的任何想法和建议对我们都非常宝贵！

## 📜 许可证 (License)

本项目采用 [MIT License](LICENSE) 开源许可证。我们稍后会在项目中添加包含完整许可证文本的 `LICENSE` 文件。

---

*如果您觉得这个项目对您有帮助，请给它一个 ⭐ Star！您的认可是我们持续改进的最大动力！*

