# app.py (Updated for Task Types including Code Generation)
import streamlit as st
import json 

try:
    import agent_logic 
    from prompt_templates import STRUCTURED_PROMPT_TEMPLATES 
except ImportError as e:
    st.error(f"å¯åŠ¨é”™è¯¯: {e}. è¯·ç¡®ä¿ agent_logic.py å’Œ prompt_templates.py æ–‡ä»¶åœ¨åŒä¸€ç›®å½•ä¸‹ã€‚")
    st.stop() 

st.set_page_config(page_title="æœ¬åœ°å…ƒæç¤ºä»£ç†", layout="wide", initial_sidebar_state="expanded")

st.title("æœ¬åœ°å…ƒæç¤ºä»£ç† (ä»»åŠ¡åŒºåˆ†ç‰ˆ) ğŸš€")
st.caption(f"ä½¿ç”¨ Ollama æ¨¡å‹: {agent_logic.OLLAMA_MODEL} | API: {agent_logic.OLLAMA_API_URL}")

# --- Session State Initialization ---
if 'processing_results' not in st.session_state:
    st.session_state.processing_results = None
if 'user_raw_request_for_feedback' not in st.session_state:
    st.session_state.user_raw_request_for_feedback = ""
if 'generated_prompt_for_feedback' not in st.session_state:
    st.session_state.generated_prompt_for_feedback = ""
if 'selected_task_type' not in st.session_state:
    st.session_state.selected_task_type = "é€šç”¨/é—®ç­”" # Default task type

# --- Sidebar for Configuration ---
with st.sidebar:
    st.header("âš™ï¸ é…ç½®é€‰é¡¹")

    # 1. é€‰æ‹©ä»»åŠ¡ç±»å‹
    task_types_available = ["é€šç”¨/é—®ç­”", "æ·±åº¦ç ”ç©¶", "å›¾åƒç”Ÿæˆ", "ä»£ç ç”Ÿæˆ"]
    # Get current index for selected_task_type to maintain selection across reruns
    current_task_index = 0
    if st.session_state.selected_task_type in task_types_available:
        current_task_index = task_types_available.index(st.session_state.selected_task_type)
    
    st.session_state.selected_task_type = st.selectbox(
        "é€‰æ‹©ä»»åŠ¡ç±»å‹:", 
        task_types_available, 
        index=current_task_index,
        key="sb_task_type"
    )
    # Extract the primary task type for logic (e.g., "é€šç”¨" from "é€šç”¨/é—®ç­”")
    current_task_type_for_logic = st.session_state.selected_task_type.split('/')[0] 

    enable_self_correction_default = True 
    enable_self_correction = st.checkbox("å¯ç”¨è‡ªæˆ‘æ ¡æ­£ (é€’å½’ä¼˜åŒ–)", value=enable_self_correction_default, key="cb_self_correct")
    
    max_recursion_depth_default = 1 
    max_recursion_depth = 0
    if enable_self_correction:
        max_recursion_depth = st.number_input("æœ€å¤§é€’å½’æ·±åº¦", min_value=0, max_value=3, value=max_recursion_depth_default, step=1, key="num_recursion_depth")

    st.subheader("ç»“æ„åŒ–æ¨¡æ¿ (å¯é€‰)")
    filtered_templates = {"æ— ": {}} 
    for name, template_info in STRUCTURED_PROMPT_TEMPLATES.items():
        template_task_categories = template_info.get("task_type", [])
        if current_task_type_for_logic in template_task_categories or \
           ("é€šç”¨" in template_task_categories and current_task_type_for_logic == "é€šç”¨") or \
           ("é—®ç­”" in template_task_categories and current_task_type_for_logic == "é—®ç­”"): # More specific matching
            filtered_templates[name] = template_info
    
    selected_template_name = st.selectbox(
        "é€‰æ‹©ä¸€ä¸ªæ¨¡æ¿ (åŸºäºä»»åŠ¡ç±»å‹ç­›é€‰):", 
        list(filtered_templates.keys()), 
        key="select_template_filtered"
    )
    
    structured_vars_input = {}
    if selected_template_name != "æ— " and selected_template_name in filtered_templates:
        st.caption(filtered_templates[selected_template_name].get("description", "æ²¡æœ‰æè¿°"))
        template_vars_needed = filtered_templates[selected_template_name].get("variables", [])
        if template_vars_needed:
            st.markdown("**æ¨¡æ¿å˜é‡:**")
            for var_name in template_vars_needed:
                # Use text_area for potentially longer inputs in specific templates/variables
                if (selected_template_name == "DetailedImageGen" and var_name == "negative_prompts") or \
                   (selected_template_name == "DetailedCodeFunction" and var_name in ["input_params", "algorithms_steps", "error_handling", "documentation_level", "dependencies", "code_style", "include_tests"]):
                     structured_vars_input[var_name] = st.text_area(f"  {var_name}:", height=80, key=f"var_{selected_template_name}_{var_name}")
                else:
                    structured_vars_input[var_name] = st.text_input(f"  {var_name}:", key=f"var_{selected_template_name}_{var_name}")
        else:
            st.info("æ­¤æ¨¡æ¿ä¸éœ€è¦é¢å¤–å˜é‡ã€‚")

# --- Main Area for Input and Output ---
st.subheader(f"1. è¾“å…¥æ‚¨çš„åˆæ­¥è¯·æ±‚ (ä»»åŠ¡ç±»å‹: {st.session_state.selected_task_type})")
user_raw_request_placeholder = "ä¾‹å¦‚ï¼šå¸®æˆ‘å†™ä¸€ç¯‡å…³äºäººå·¥æ™ºèƒ½åœ¨åŒ»ç–—é¢†åŸŸåº”ç”¨çš„åšå®¢æ–‡ç« å¤§çº²ã€‚"
if current_task_type_for_logic == "å›¾åƒç”Ÿæˆ":
    user_raw_request_placeholder = "ä¾‹å¦‚ï¼šä¸€åªåœ¨æœˆçƒä¸Šçœ‹ä¹¦çš„çŒ«ï¼Œèµ›åšæœ‹å…‹é£æ ¼ã€‚"
elif current_task_type_for_logic == "ç ”ç©¶":
    user_raw_request_placeholder = "ä¾‹å¦‚ï¼šåˆ†ææ°”å€™å˜åŒ–å¯¹å†œä¸šçš„é•¿æœŸå½±å“ã€‚"
elif current_task_type_for_logic == "ä»£ç ç”Ÿæˆ":
    user_raw_request_placeholder = "ä¾‹å¦‚ï¼šç”¨Pythonå†™ä¸€ä¸ªå‡½æ•°ï¼Œè®¡ç®—æ–æ³¢é‚£å¥‘æ•°åˆ—çš„ç¬¬né¡¹ï¼Œéœ€è¦å¤„ç†è´Ÿæ•°è¾“å…¥ã€‚"


user_raw_request = st.text_area("åœ¨æ­¤å¤„è¾“å…¥æ‚¨çš„è¯·æ±‚:", height=150, key="text_user_request",
                                placeholder=user_raw_request_placeholder)

if st.button("ğŸš€ ç”Ÿæˆä¼˜åŒ–æç¤ºè¯", key="btn_generate", type="primary", use_container_width=True):
    # Validation logic
    proceed_with_generation = False
    if selected_template_name != "æ— ": # If a structured template is selected
        template_vars_needed = filtered_templates.get(selected_template_name, {}).get("variables", [])
        all_vars_filled = True
        if template_vars_needed: # Only check if template actually has variables defined
            for var_name in template_vars_needed:
                if not structured_vars_input.get(var_name, "").strip():
                    st.error(f"ç»“æ„åŒ–æ¨¡æ¿ '{selected_template_name}' éœ€è¦å˜é‡ '{var_name}' çš„å€¼ã€‚")
                    all_vars_filled = False
                    break
        if all_vars_filled:
            proceed_with_generation = True
    elif user_raw_request.strip(): # No template, but raw request is present
        proceed_with_generation = True
    else: # No template and no raw request
        st.warning("è¯·è¾“å…¥æ‚¨çš„åˆæ­¥è¯·æ±‚ï¼Œæˆ–é€‰æ‹©ä¸€ä¸ªç»“æ„åŒ–æ¨¡æ¿å¹¶å¡«å†™å…¶å˜é‡ã€‚")

    if proceed_with_generation:
        st.session_state.processing_results = None 
        with st.spinner(f"ğŸ¤– ä»£ç†æ­£åœ¨ä¸º '{st.session_state.selected_task_type}' ä»»åŠ¡æ€è€ƒä¸­..."):
            use_template_for_logic = selected_template_name if selected_template_name != "æ— " else None
            results = agent_logic.generate_and_refine_prompt(
                user_raw_request=user_raw_request, 
                task_type=current_task_type_for_logic,
                enable_self_correction=enable_self_correction,
                max_recursion_depth=max_recursion_depth,
                use_structured_template_name=use_template_for_logic,
                structured_template_vars=structured_vars_input if use_template_for_logic else None
            )
            st.session_state.processing_results = results
            st.session_state.user_raw_request_for_feedback = user_raw_request # Store the raw request for feedback context
            st.session_state.generated_prompt_for_feedback = results.get("final_prompt", "")


# --- Display Results ---
if st.session_state.processing_results:
    results = st.session_state.processing_results
    st.divider()
    st.subheader(f"2. å¤„ç†ç»“æœ (ä»»åŠ¡: {st.session_state.selected_task_type})")

    if results.get("error_message"):
        st.error(f"å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {results['error_message']}")
        if results.get("error_details"):
            st.json(results["error_details"]) 
    else:
        st.success("ğŸ‰ æç¤ºè¯å·²æˆåŠŸç”Ÿæˆ/ä¼˜åŒ–ï¼")

        with st.expander("æŸ¥çœ‹è¯¦ç»†å¤„ç†æ—¥å¿—å’Œä¸­é—´æ­¥éª¤", expanded=False):
            st.markdown("#### å†…éƒ¨å¤„ç†æ—¥å¿—:")
            log_container = st.container(height=200) 
            for log_entry in results.get("log", []):
                log_container.text(log_entry) 
            
            st.markdown("#### åˆå§‹æ ¸å¿ƒæç¤º (å‘é€ç»™LLM):")
            st.text_area("åˆå§‹æ ¸å¿ƒæç¤ºå†…å®¹:", value=results.get("initial_core_prompt", "æœªè®°å½•"), height=200, disabled=True, key="disp_initial_core")

            st.markdown("#### åˆæ­¥ä¼˜åŒ–åçš„æç¤ºè¯ (P1):")
            st.text_area("P1å†…å®¹:", value=results.get("p1_initial_optimized_prompt", "æœªç”Ÿæˆæˆ–æœªè®°å½•"), height=200, disabled=True, key="disp_p1")


            if results.get("evaluation_reports"):
                for i, report in enumerate(results["evaluation_reports"]):
                    st.markdown(f"#### è¯„ä¼°æŠ¥å‘Š (E{i+1}):")
                    st.text_area(f"E{i+1}å†…å®¹:", value=report, height=200, disabled=True, key=f"disp_e{i+1}")
            
            if results.get("refined_prompts"):
                for i, prompt_text in enumerate(results["refined_prompts"]):
                    st.markdown(f"#### ç¬¬ {i+1} è½®ç²¾ç‚¼åçš„æç¤ºè¯ (P{i+2}):")
                    st.text_area(f"P{i+2}å†…å®¹:", value=prompt_text, height=200, disabled=True, key=f"disp_p_refined{i+1}")
        
        st.subheader("ğŸ¯ æœ€ç»ˆä¼˜åŒ–åçš„ç›®æ ‡æç¤ºè¯:")
        final_prompt_text = results.get("final_prompt", "æœªèƒ½ç”Ÿæˆæœ€ç»ˆæç¤ºè¯ã€‚")
        # Allow copying by making it a non-disabled text_area
        st.text_area("æœ€ç»ˆæç¤ºè¯å†…å®¹ (å¯å¤åˆ¶):", value=final_prompt_text, height=300, key="disp_final_prompt_copyable", help="æ‚¨å¯ä»¥ä»æ­¤æ¡†ä¸­å¤åˆ¶æç¤ºè¯ã€‚")
        
        st.divider()
        st.subheader("3. æä¾›åé¦ˆ (å¯é€‰)")
        if st.session_state.generated_prompt_for_feedback and "é”™è¯¯ï¼š" not in st.session_state.generated_prompt_for_feedback :
            feedback_rating = st.slider("æ‚¨å¯¹è¿™ä¸ªæç¤ºè¯çš„è´¨é‡è¯„åˆ† (1=å·®, 5=ä¼˜):", 1, 5, 3, key="slider_rating_tasks")
            feedback_comments = st.text_area("æ‚¨çš„å…·ä½“ä¿®æ”¹å»ºè®®æˆ–è¯„è®º:", key="text_feedback_comments_tasks", height=100)
            
            if st.button("æäº¤åé¦ˆ", key="btn_submit_feedback_tasks"):
                feedback_to_save = {
                    "rating": feedback_rating,
                    "comments": feedback_comments,
                    "original_request": st.session_state.user_raw_request_for_feedback,
                    "generated_prompt": st.session_state.generated_prompt_for_feedback,
                    "task_type": st.session_state.selected_task_type, 
                    "model_used": agent_logic.OLLAMA_MODEL, 
                    "self_correction_enabled": enable_self_correction, 
                    "recursion_depth_if_enabled": max_recursion_depth if enable_self_correction else 0,
                    "structured_template_used": selected_template_name if selected_template_name != "æ— " else "æ— "
                }
                
                current_feedback_list = agent_logic.load_feedback()
                current_feedback_list.append(feedback_to_save)
                if agent_logic.save_feedback(current_feedback_list):
                    st.success("æ„Ÿè°¢æ‚¨çš„åé¦ˆï¼å·²ä¿å­˜ã€‚")
                else:
                    st.error("ä¿å­˜åé¦ˆå¤±è´¥ã€‚è¯·æ£€æŸ¥åå°æ—¥å¿—ã€‚")
        else:
            st.info("ç”Ÿæˆæç¤ºè¯åå¯åœ¨æ­¤å¤„æä¾›åé¦ˆã€‚")

st.sidebar.divider()
st.sidebar.info(
    "è¿™æ˜¯ä¸€ä¸ªæœ¬åœ°å…ƒæç¤ºä»£ç†çš„æ¼”ç¤ºç•Œé¢ã€‚\n\n"
    "å®ƒä½¿ç”¨Ollamaåœ¨æ‚¨çš„è®¡ç®—æœºä¸Šæœ¬åœ°è¿è¡ŒLLMã€‚"
)

